In the next few lectures, we'll be creating your very first streaming application using the kafka

Streams Library your API, kafka Streams Library is a abstraction built on top of the Kafka claims

library transitively pulls it and uses the Kafka Claims Library.

And it makes our life as a developer super easy to create

Streaming applications. We will be working on a dataflow use case where the data will flow as a stream from

a input topic to an output tropic.

And in between, those two will apply whatever transformational logic you want.

To create any streaming application, there are four simple steps.

The first step is to create and configure the required properties there are four mandatory properties.

All the properties are defined in the streams config class as constants.

The first one is a unique application ID, which we need to provide for every streaming application.

Second is the bootstrap server config

Just like how we have used the producers and consumers earlier.

It points to the broker details.

Then we have two new properties which are completely different from the producers and consumers we have

created earlier.

But they are related.

Default key underscores when you have worked with producers and consumers earlier, separately using

the Kafka client library on the producer side, we used to configure the serializer, the key serializers and

the value serializers.

On the consumer side, we used to define the key serializers and value de serializers , but a streaming

application is both a consumer and a producer.

So we need to provide a serde class serializer and deserializer class.

The Kafka Stream's library makes it super easy to define and use a serde class, it gives us a factory

class called Serdes.

This factory class has various methods.

When invoked, will return the appropriate serde class for us.

So if you invoke serdes dot string, that will return us a serializer and deserializer class a serde

that can handle the string type of data.

If you want long you invoke the long if you want double, you invoke the double and so on.

If you look at the string method internally.

You will see that internally this serdes string serdes are just wrapper around the serializer and deserializers

which you have used earlier, the string serdes uses the string serializer and string serializer.

Similarly, the other types the float the double, they use the double and double deseializers.

These are from the Kafka Client Library, which you have already used this from org Apache Kafka Common

Serialisation String Serializer.

And similarly deserializer

So this API makes it easy for us to define a serde class, which can handle both serialisation and deserialization

of a key similarly the serialization and deserialization of a value.

The serdes is a factory class and whatever type method is invoked that type of serde will be returned and

used.

So that is the first key step.

Next is the heart of our streaming application, where we define the computational logic or the topology

for our application to create a topology.

We use the Streams Builder class.

You can also look at it as a topology builder.

Using that builder instance, we start streaming the data or reading the data from a input topic, typically a stream

starts from input topic and it ends with the output topic where the data goes to a output topic

And in between those two will be applying various transformational logic.

So this builder dot stream returns a kafka stream object, and this object has various methods which will

transform the data.

Whatever logic you want to apply.

You can apply.

You can loop through them.

Apply the logic on keys and values.

You can filter the data by passing in a predicate.

You can map by using the map method.

You can convert the keys and values to a completely different types of records with keys and values.

You you're going to explore more of all that in the next few lectures.

So the second step is to create a topology or the computational logic.

We build it using the stream builder and you say builder dot build using all this logic

you have define here.

A topology object will be returned.

Third step is to create a kafka streams.

This is the client, the Kafka.

Client steam's client object, you pass it that the topology and the props, in the previous two

steps, whatever you have got, the properties and the topology, you pass it to the client and then

start the stream.

Only when you invoke the start method your stream will start flowing.

The data will start flowing from one topic.

This computational logic will be applied and then it will be returned to another output topic.

And last and very important step.

We need to shut down or close the stream.

Whatever we have started, we will close it whenever this program is stopped.

So by default, when you invoke the streams dot start, it will go into an infinite loop.

The stream will be flowing only when you stop this program by hitting the stop button in your ide or control

c on the console, it will stop.

The stream will stop.

So we want to attach a shut down hook whenever this process stops, we want to close the stream.

That is the reason we will use run time

Dot get runtime, which will give us the runtime of this client program or the stream program.

We add a shutdown hook.

To which we pass a new threat.

Which simply closes the stream here, invokes the streams dot close, so in the next few lectures,

you're going to use the kafka streams library and build this data flow stream one step at a time.

